# 고급 정렬   
### 1. 병합정렬 

(1) 실행 과정 요약  
**merge sort 알고리즘**  
```
mergeSort(A[ ], start, end) 
{ 
        if (start < end) then {                 // 정렬할 부분의 크기가 1 이하이면, 그냥 리턴한다.
                middle ← (start + end)/2;       // (1) start, end의 중간 지점 계산 - 상수
                mergeSort(A, start, middle);    // (2) 중간 지점을 기준으로 앞부분 정렬 - log n
                mergeSort(A, middle+1, end);    // (3) 중간 지점을 기준으로 뒷부분 정렬 
                merge(A, start, middle, end);   // (4)  병합 - n
                // 결과적으로 n log n이다.
        } 
} 

merge(A[ ], start, middle, end) 
{ 
        정렬되어 있는 두 배열 A[start ... middle]와 A[middle+1 ... end]을 합하여 
        정렬된 하나의 배열 A[start ... end]을 만든다. 
} 
```  

if(strat == end)이면, 정렬할 부분의 크기가 1 이다. return 한다.   
if(start < end)이면, 정렬할 부분의 크기가 2 이상이다.   

예) a = {31, 3, 65, 73, 8, 11}   
mergeSort(a, 0, a.length-1);  

[31 3 65 73 8 11] mergeSort(a, 0, 5) 호출, middle = 2  
mergeSort(a, 0, 2) 호출 [31 3 65] [73 8 11]  
middle = 1, mergeSort(a, 0, 1) 호출 [31 3] [65]  
middle = 0, mergeSort(a, 0, 0) 호출 return   
mergeSort(a, 1, 1) 호출 return   
merge(1, 0, 0, 1) 호출   
merge 후 배열 : a {3, 31, 65, 73, 8, 11}  
 
mergeSort(a, 2, 2) return    
merge(a, 0, 1, 2) 호출      
merge 후 배열 : a {3, 31, 65, 73, 8, 11}     
(이후 생략)

[코드 보러가기](https://github.com/hyerin6/algorithm/blob/master/algorithm1/src/sort/merge/Example1.java)  

(2) 병합 정렬 수행 시간   
**수행 시간**  
merge 메소드의 수행 시간은 O(n)    
배열을 1/2로 나누고, 나뉜 배열 각각에 대해서 mergeSort 재귀 호출하는 횟수는 logn    
mergeSort 메소드의 수행시간은 O(n log n)     

**메모리 공간**  
merge 메소드는 병합할 배열과 동일한 크기의 임시 배열을 생성한다.  
따라서 입력 배열을 제외하고, mergeSort 메소드가 사용하는 메모리 공간은 O(n) 이다.  

**Q.** temp 배열 없이 merge를 구현하면 안되나요?    
**A.** 배열의 값을 계속 어딘가에 순서대로 저장해야 되기 때문에 (여분의 공간이 여러개 필요하다.)     
   배열을 생성하는 것과 비슷한 메모리가 필요하다.    
   

### 2. 퀵 정렬    

(1) 실행 과정 요약     
**partition**  
기준값 : 입력 배열의 끝 값   
1구역 : 기준 값보다 작거나 같은 값들이 위치  
2구역 : 기준 값보다 큰 값들이 위치  
3구역 : 아직 비교하지 않아 위치가 정해지지 않은 값  
```   
partition(a[ ], p, r){  // 기준 원소(pivot = p)
  x = a[r];       
  i = p - 1;            // i는 1구역의 끝
  for(j = p; to r-1)    // j는 3구역의 시작 
    if(a[j] < x)        // a[j] 값이 1구역에 속하면
    // 위 if문은'작거나 같다' 보다 '작다'로 구현하는 것이 더 나음
      swap(a, ++i, j);  // a[j] 값을 1구역의 끝에 추가한다. - i 값 증가 후 A[i] ↔ A[j] 교환
  swap(a, i+1, r);      // 기준 원소와 2구역 첫 원소 교환
  return i + 1;         // 기준 원소의 위치 리턴
}
```  

**코드 분석**  
**Q.** 기준값이 최대/최소 값일 때, partition 결과는 어떻게 되는가?  
**A.** 분할은 0개와 나머지 전체로 나누어지므로 결국 아무변화가 없고 똑같은 반복이 이루어지므로 시간복잡도가 O(n^2)이 된다.  

예) a = {31, 8, 48, 73, 11, 3, 20, 29, 65, 15}     
partotion(a, 0, 9) 기준값 = 15    

① j=0, i=-1 if문 false  

② j=1, i=-1 if문 true - swap(a, ++i, j)  
a[i+1] 값도 1구역에 포함되어야 하므로, ++i 해야 한다.    
결과 : {8, 31, 48, 73, 11, 3, 20, 29, 65, 15}     

(if문 false 생략)    

③ j=4, i=0, if문 true - swap(a, ++i, j)    
결과 : {8, 11, 48, 78, 31, 3, 20, 29, 65, 15}     

④ j=5, i=1, if문 true - swap(a, ++i, j)    
결과 : {8, 11, 3, 78, 31, 48, 20, 29, 65, 15}     

⑤  for문 종료, 기준값과 2구역 시작 값 교환, i가 1구역의 끝이기 때문에 swap(a, i+1, end)    
결과 : {8, 11, 3, 15, 31, 48, 20, 29, 65, 73}    

여기서 1구역, 2구역을 각각 따로 정렬하면, 배열 전체 정렬이 완료된다.    
이미 기준값은 정렬이 완료된 상태이기 때문에 기준값이 위치한 곳은 정렬에서 제외된다.    
(partition 메소드는 기준값의 위치를 리턴한다.)   
[코드 보러가기](https://github.com/hyerin6/algorithm/blob/master/algorithm1/src/sort/quick/Example2.java)

(2) 퀵 정렬 수행 시간  
partition 메소드의 수행 시간은 O(n) 이다.  

- 평균   
quickSort 메소드의 재귀 호출 횟수는, 대략 logn 이다.  
따라서 quickSort 메소드의 수행시간은 O(n log n) 이다.  

- best case  
partition 메소드가 배열을 정확히 1/2로 나눈다면, 재귀 호출 횟수는 logn 이고,  
quickSort 메소드의 수행 시간은 O(n log n) 이다.  

- worst case   
partition 메소드가 배열을 0 : n-1 크기로 나눈다면,   
(재귀 호출에서 기준값이 제외되기 때문에, 0 : n 이 아니고, 0 : n-1 이다.)  
재귀 호출 횟수는 n 이다.  
quickSort 메소드의 worst case 수행 시간은 O(n2) 이다.  
기준값이 최소, 최대일 때 배열의 크기가 n-1이 된다.  

- 메모리 공간  
quickSort, partition 메소드가 사용하는 메모리 공간은 O(1) 이다.  

(3) 퀵 정렬과 병합 정렬 비교  
병합 정렬의 수행시간은 언제나 O(n log⁡n)이다.  
병합 정렬의 메모리 요구량은 O(n) 언제나 이다.  

퀵 정렬의 수행시간은 평균 O(n log⁡n) 이지만, worst case O(n^2) 이다. (Merge 정렬이 평균적으로 수행 시간이 느리지만)  
그렇지만 worst case는 극히 드물다. (최악의 경우는 드물다. – 이미 정렬된 배열)  
퀵 정렬의 메모리 요구량은 O(1) 이다.  

위와 같은 장단점 때문에, 실무에서는 주로 퀵 정렬을 사용한다.  

### 3. 힙 정렬  
(1) 개념  
다음 성질을 만족하는 이진 트리(binary tree)를 힙(heap)이라고 부른다.  
- 맨 아래 계층은 자식이 없다.  
- 맨 아래 계층과 그 바로 위 계층을 제외한 노드는 자식이 두 개이다. –> 채워진 이진트리  
- 맨 아래 계층은 왼쪽부터 꽉 채워져 있다.  
- 노드의 값은 자식 노드들의 값보다 작다. -> 왼쪽 오른쪽 구분 없음  

**힙의 예**  
<img width="437" alt="스크린샷 2019-04-05 오후 12 51 14" src="https://user-images.githubusercontent.com/33855307/55602633-8852f500-57a1-11e9-8b9a-54b7a7f13f2d.png">
배열에 저장할 떄 공간 낭비를 하지 않을 수 있다.    

**이진트리의 장단점**  
장점 : 자식노드로 부모노드를 찾을 수 있다.  
단점 : 힙이 아니면 공간 낭비가 심하다. 

<br /><br />


(2) 힙 배열에 저장하기  
- 인덱스 k 노드의 왼쪽 자식의 인덱스 =  2k + 1   -> 홀수  
- 인덱스 k 노드의 오른쪽 자식의 인덱스 = 2k + 2   -> 짝수   
- 인덱스 k 노드의 부모의 인덱스 (주어진 노드의 부모를 찾고 싶다면 그냥 (k-1)/2 해주면 됨)     
        k 노드가 왼쪽 자식인 경우 (k - 1)/2, k 노드가 오른쪽 자식인 경우 (k - 2)/2      


(3) heapsort 사례 - 생략  

<br /><br />

(4) 규칙 정리  
**자식이 있는 마지막 노드의 인덱스는?**  
인덱스 k 노드의 왼쪽 자식의 인덱스 =  2k + 1  
인덱스 k 노드의 오른쪽 자식의 인덱스 = 2k + 2  
인덱스 k 노드의 부모 인덱스 = (k - 1) / 2  

배열의 마지막 인덱스를 end 라고 할 때,  
end 인덱스가 짝수이면 오른쪽 자식이고, 홀수이면 왼쪽 자식이다.  

부모 노드의 인덱스를 k 라고 할 때,  
end 인덱스가 짝수이면, end = 2k + 2 이고, k = (end - 2) / 2 이다.  
end 인덱스가 홀수이면, end = 2k + 1 이고, k = (end - 1) / 2 이다.  

배열의 마지막 인덱스를 end 라고 할 때,  
자식이 있는 마지막 노드의 인덱스는 (end - 1) / 2 이다.  


**heap 조정 규칙 #1**  
heap의 끝 값을 루트로 이동한 후, heap의 성질이 만족되도록 조정하는 절차는 다음과 같다.  
```
// 주어진 인덱스만 힙의 성질에 만족하지 못하고 나머지는 힙에 만족한다.  
heapify(int[] a, int index) {  // index 위치의 노드가 heap 성절을 만족하도록 조정
    자식이 없다면 리턴 // 종료 조건

    왼쪽 자식 값과 오른쪽 자식 값 중에서 작은 값을 구한다.  // 현재 단계의 작업
    그 작은 값보다 index 위치의 값이 더 작다면 리턴
    index 위치의 값이 더 크다면, 두 값을 swap 한다.
    
    heapify(a, 작은자식index)  // 다음 단계 재귀 호출
}

```   

**heap 조정 규칙 #2**    
절차를 조금 더 구체적으로 정리해보자.  
heap은 complete binary tree이므로, 임의의 노드의 자식의 수는  
두 자식 다 있거나, 왼쪽 자식만 있거나, 자식이 없거나 이다.  

두 자식이 다 있는 경우,  왼쪽 자식 인덱스 < 오른쪽 자식 인덱스 <= **마지막 인덱스**  
왼쪽 자식만 있는 경우, 왼쪽 자식 인덱스 <= **마지막 인덱스** < 오른쪽 자식 인덱  
자식이 없는 경우, **마지막 인덱스** < 왼쪽 자식 인덱스 < 오른쪽 자식 인덱스  

```
heapify(int[] a, int index) {  // index 위치의 노드가 heap 성절을 만족하도록 조정
    leftChildIndex = 2 * index + 1
    rightChildIndex = 2 * index + 2
    endIndex = a.length - 1

    // 자식이 없다면 리턴.  종료 조건. 
    if (endIndex < leftChildIndex) return

    // 자식이 둘 다 있다면, smallIndex를 구한다
    if (rightChildIndex <= endIndex)
        smallIndex = a[leftChildIndex] < a[rightChildIndex] ? leftChildIndex : rightChildIndex
    // 왼쪽 자식만 있다면, 왼쪽 자식이 smallIndex
    else if (leftChildIndex <= endIndex)
        smallIndex = leftChildIndex
    // 자식이 없다면 리턴
    else return

    // smallIndex 값보다 index 값이 더 크면 swap
    if (a[index] > a[smallIndex]) {
        swap(a, index, smallIndex)        
        heapify(a, smallIndex) // 다음 단계 재귀 호출
    }
}
``` 
                           

**처음 주어진 배열을 heap 만들기**  
처음에 주어진 배열 전체를 heap으로 만들려면, 트리의 아래쪽부터 위로 heapify 호출해야 한다.  
자식이 없는건 힙이 맞기 때문에 자식이 있는 노드부터 호출해서 정리하면 된다.   

배열의 마지막 인덱스를 end 라고 할 때,  
자식이 있는 마지막 노드의 인덱스는 (end - 1) / 2 이다.  

for (index = (end - 1)/2; index >=0; --index)  
    heapify(a, index);  
    
    
- 힙 정렬 규칙 #1  
```
처음 주어진 배열을 heap 만든다.

end = 배열의 마지막 인덱스
while (end > 0) {
    swap(배열, 0, end)    // heap의 루트 항목을 추출한다.
    end = end - 1       // heap의 끝 항목을 root 위치로 이동한다.
    heapify(a, 0, end) 
}
```  


(5) 힙 정렬 수행 시간  
**heapify 함수 수행 시간**  
heapify 함수는 이진 트리에서 자식 노드와 순서를 바꾸는 재귀호출을 한다.  
힙 이진 트리의 높이는 logN 이다.  
따라서 heapify 함수의 재귀호출 횟수는 logN  이다.  
따라서 heapify 함수의 수행 시간은 O(logN) 이다.  

**buildHeap 함수 수행 시간**  
buildHeap 함수에는 N에 비례하는 for 루프가 있다.  
이 for 루프에서 heapify 함수를 호출한다.  
따라서 buildHeap 함수의 수행 시간은 O(N logN) 이다.  

**heapSort 함수 수행 시간**  
buildHeap 함수를 1회 호출한다.  
heapify 함수를 N회 호출한다.  
따라서 heapSort 함수의 수행 시간은 O(N logN) 이다.   

